{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71pwnb6wII8S"
      },
      "source": [
        "# **üöÄ LLaVA Model Deployment Notebook**\n",
        "\n",
        "## üìñ **Overview**\n",
        "Build a robust, production-ready pipeline that seamlessly combines powerful vision-language AI with scalable web deployment and e-commerce listing generation:\n",
        "\n",
        "1. **Quantized LLaVA-Next / Mistral-7B Model** ‚Äî Efficient multi-modal understanding  \n",
        "2. **FastAPI Server** ‚Äî Lightning-fast, production-grade API  \n",
        "3. **Ngrok Tunneling** ‚Äî Secure public endpoint for easy access  \n",
        "4. **Multi-Image Product Analysis** ‚Äî Detailed JSON output for rich insights  \n",
        "5. **Mistral API Integration** ‚Äî Automated generation of polished e-commerce listings  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVtsYKZBII8V"
      },
      "source": [
        "## **üöÄ Environment Setup**\n",
        "\n",
        "Prepare your system with all necessary packages for lightning-fast model inference, quantization, and API hosting.  \n",
        "We'll also clean up any old `transformers` versions to keep things smooth, then grab the latest cutting-edge release directly from GitHub! ‚ö°‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEU2IUS2BJ-4"
      },
      "outputs": [],
      "source": [
        "# System packages installation\n",
        "!pip install -q accelerate transformers peft bitsandbytes trl flash-attn xformers\n",
        "!pip install -q fastapi nest-asyncio pyngrok uvicorn python-multipart\n",
        "\n",
        "# Clean up existing transformers installation\n",
        "!pip uninstall -y transformers\n",
        "\n",
        "# Install latest transformers from source\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_R78Lp6II8X"
      },
      "source": [
        "## üîê **Hugging Face Authentication**\n",
        "\n",
        "Configure access to Hugging Face models:\n",
        "\n",
        "- üîë **Token Setup**: Authenticate using your personal access token.\n",
        "- üåê **Secure Access**: Enables downloading and using gated or private models from Hugging Face Hub.\n",
        "- ‚ö° **One-Time Configuration**: Required only once per session or environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xq9oTC7AU-a"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder\n",
        "import os\n",
        "\n",
        "# Configure HF authentication\n",
        "HfFolder.save_token(\"hf_YOUR_TOKEN_HERE\")  # Replace with actual token\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN_HERE\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oh6w9oNII8Y"
      },
      "source": [
        "## ‚öôÔ∏è **Model Initialization**\n",
        "\n",
        "This section sets up the core models with performance-focused configurations:\n",
        "\n",
        "- üß© **LLaVA-Next / Mistral-7B Quantized**: Loads the visual language model with quantized weights for reduced memory and faster inference.\n",
        "- üîß **Optimized Pipeline**: Ensures efficient tokenizer, processor, and device assignment (e.g., GPU/CPU).\n",
        "- üöÄ **Ready for Inference**: Models are configured to run in low-resource environments while maintaining output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF0GGDIR4T2a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, BitsAndBytesConfig\n",
        "from PIL import Image\n",
        "\n",
        "# Environment configuration\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64,garbage_collection_threshold:0.7\"\n",
        "os.environ[\"TRANSFORMERS_NO_FLASH_ATTENTION\"] = \"1\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Model initialization\n",
        "processor = LlavaNextProcessor.from_pretrained(\n",
        "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "# Compilation optimization\n",
        "if hasattr(torch, \"compile\"):\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rozbg98_II8Y"
      },
      "source": [
        "## üöß **Inference Pipeline**\n",
        "\n",
        "This section implements the core logic for:\n",
        "\n",
        "- üì∏ **Image Preprocessing**: Resizes and prepares multiple input images for model inference.\n",
        "- üß† **Prompt Construction**: Dynamically formats the input text prompt for visual grounding.\n",
        "- üó£Ô∏è **Text Generation**: Uses the quantized LLaVA model to generate structured output based on visual inputs.\n",
        "\n",
        "Everything is optimized for fast batch processing and maximum token efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inJWrkqm57nQ"
      },
      "outputs": [],
      "source": [
        "def generate_fast(image_paths, prompt,\n",
        "                  max_new_tokens=1024,\n",
        "                  num_beams=1,\n",
        "                  temperature=1.0,\n",
        "                  img_size=160):\n",
        "    # Build the full prompt by inserting one <image> token per image before the actual prompt\n",
        "    image_token = processor.tokenizer.image_token or \"<image>\"\n",
        "    placeholders = image_token * len(image_paths)\n",
        "    full_prompt = f\"{placeholders} {prompt}\"\n",
        "\n",
        "    # Load and preprocess all images: convert to RGB and resize to (img_size, img_size)\n",
        "    images = [\n",
        "        Image.open(p).convert(\"RGB\").resize((img_size, img_size), Image.LANCZOS)\n",
        "        for p in image_paths\n",
        "    ]\n",
        "\n",
        "    # Tokenize and batch the input prompt and images\n",
        "    inputs = processor(images=images, text=full_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate model output using the given generation parameters\n",
        "    with torch.inference_mode():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=num_beams,\n",
        "            temperature=temperature,\n",
        "            early_stopping=True,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    # Decode and return the generated text output\n",
        "    return processor.decode(out_ids[0], skip_special_tokens=True).strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPNQ0pWYII8Z"
      },
      "source": [
        "## üöÄ **API Endpoint:** `/analyze-product`\n",
        "\n",
        "This FastAPI endpoint handles the full flow for automated product listing generation:\n",
        "\n",
        "- **Accepts**: Multiple uploaded product images via `POST`.\n",
        "- **Processes**:\n",
        "  - Saves images temporarily.\n",
        "  - Sends them to the `generate_fast()` function (LLaVA model) to extract structured product details.\n",
        "- **Chains to**: A Mistral backend (`/generate-listing`) to turn raw details into a formatted product listing.\n",
        "- **Returns**: Final cleaned JSON object (`listing_section`) containing:\n",
        "  - `Title`\n",
        "  - `Category`\n",
        "  - `Attributes`\n",
        "  - `KeyFeatures`\n",
        "  - `UseCase`\n",
        "  - `Perfectly Accurate Detailed Description`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "024L2kbElOTC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define a temporary directory for image uploads\n",
        "tmp_dir = \"tmp_uploads\"\n",
        "\n",
        "@app.post(\"/analyze-product\")\n",
        "async def analyze_product(images: list[UploadFile] = File(...)):\n",
        "    os.makedirs(tmp_dir, exist_ok=True)\n",
        "    paths = []\n",
        "\n",
        "    try:\n",
        "        # Step 1: Save uploaded image files to disk\n",
        "        for img_file in images:\n",
        "            contents = await img_file.read()\n",
        "            tmp_path = os.path.join(tmp_dir, f\"{img_file.filename}\")\n",
        "            with open(tmp_path, 'wb') as f:\n",
        "                f.write(contents)\n",
        "            paths.append(tmp_path)\n",
        "\n",
        "        # Step 2: Run LLaVA to extract visual information from images\n",
        "        PROMPT = (\n",
        "            \"Extract everything you can see in the image(s) and output exactly one JSON object \"\n",
        "            \"with keys: Title, Category, Attributes, KeyFeatures, UseCase, Perfectly Accurate Detailed Description. \"\n",
        "            \"Only include details you can confirm visually.\"\n",
        "            \"Format the JSON properly with double quotes.\"\n",
        "        )\n",
        "        output = generate_fast(paths, PROMPT)\n",
        "\n",
        "        # Log LLaVA output for debugging\n",
        "        print(\"\\n=== LLaVA RAW OUTPUT ===\\n\", output, \"\\n=========================\\n\")\n",
        "\n",
        "        # Step 3: Send LLaVA output to Mistral API for further generation\n",
        "        mistral_url = os.getenv(\"MISTRAL_API_URL\", \"MISTRAL-MODEL-NGROK-API-URL\").rstrip(\"/\") + \"/generate-listing\"\n",
        "        try:\n",
        "            resp = requests.post(mistral_url, json={\"llava_output\": output})\n",
        "            mistral_output = resp.json()\n",
        "        except Exception as e:\n",
        "            mistral_output = {\"error\": f\"Failed to call Mistral: {e}\"}\n",
        "\n",
        "        # Log Mistral output for debugging\n",
        "        print(\"\\n=== MISTRAL LISTING OUTPUT ===\\n\", mistral_output, \"\\n=============================\\n\")\n",
        "\n",
        "        # Step 4: Extract only the final JSON listing section from Mistral's output\n",
        "        raw_text = mistral_output.get('raw_mistral_output', \"\")\n",
        "        last_title_idx = raw_text.rfind('\"title\"')\n",
        "\n",
        "        if last_title_idx != -1:\n",
        "            start_idx = raw_text.rfind('{', 0, last_title_idx)\n",
        "            end_idx = raw_text.rfind('}')\n",
        "            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
        "                listing_section = raw_text[start_idx:end_idx + 1]\n",
        "            else:\n",
        "                listing_section = raw_text\n",
        "        else:\n",
        "            listing_section = raw_text\n",
        "\n",
        "        print(\"=== LISTING SECTION ===\", listing_section, \"========================\")\n",
        "\n",
        "        # Step 5: Return the structured listing section as JSON\n",
        "        return JSONResponse({\n",
        "            \"listing_section\": listing_section\n",
        "        })\n",
        "\n",
        "    finally:\n",
        "        # Step 6: Clean up uploaded image files\n",
        "        for p in paths:\n",
        "            if os.path.exists(p):\n",
        "                os.remove(p)\n",
        "\n",
        "\n",
        "# Start FastAPI server with ngrok tunneling\n",
        "NGROK_TOKEN = \"YOUR-NGROK-TOKEN-HERE\"  # Replace with your own\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f'üåê LLaVA API URL: {ngrok_tunnel.public_url}')\n",
        "\n",
        "# Enable asyncio compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run FastAPI app using Uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
