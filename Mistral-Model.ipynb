{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf106a45",
      "metadata": {
        "id": "cf106a45"
      },
      "source": [
        "# üöÄ **Mistral Deployment Notebook**\n",
        "\n",
        "A complete pipeline covering:\n",
        "\n",
        "- ü§ñ **Model Setup:** Load and quantize Ministral-8B  \n",
        "- üìù **Prompting:** Define and apply your product listing template  \n",
        "- ‚ö° **Generation:** Use LangChain LLMChain to create JSON listings  \n",
        "- üåê **API & Deployment:** FastAPI with ngrok for instant public access  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967f3dc0",
      "metadata": {
        "id": "967f3dc0"
      },
      "source": [
        "## üõ†Ô∏è **Install Dependencies**\n",
        "\n",
        "Make sure to install all necessary packages for smooth operation:\n",
        "\n",
        "- ‚öôÔ∏è **Model Quantization**: Support for efficient Mistral model loading.  \n",
        "- üîó **LangChain**: For prompt templating and LLM chaining.  \n",
        "- üöÄ **API Deployment**: FastAPI and uvicorn for serving the application.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917928f7",
      "metadata": {
        "id": "917928f7"
      },
      "outputs": [],
      "source": [
        "# Core LLM and quantization libraries\n",
        "!pip install -q -U langchain transformers bitsandbytes accelerate optimum langchain_community\n",
        "\n",
        "# Web framework and async tooling for API server\n",
        "!pip install -q fastapi nest-asyncio python-multipart uvicorn\n",
        "\n",
        "# Ngrok for tunneling your local server to a public URL\n",
        "!pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8785ea73",
      "metadata": {
        "id": "8785ea73"
      },
      "source": [
        "## üì• **Import Required Libraries**\n",
        "\n",
        "Load essential Python packages for model handling, API, and utilities:\n",
        "\n",
        "- ü§ó **Transformers**: For loading and managing language models.  \n",
        "- üöÄ **FastAPI**: To create the web API endpoint.  \n",
        "- üåê **ngrok**: To expose the local server publicly.  \n",
        "- üìö **LangChain & Pydantic**: For prompt management and data validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745b452b",
      "metadata": {
        "id": "745b452b"
      },
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import torch                      # PyTorch for model operations\n",
        "import json                       # Handling JSON data\n",
        "import re                         # Regular expressions for text parsing\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# LangChain components for LLM pipelines\n",
        "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
        "\n",
        "# FastAPI for building API\n",
        "from fastapi import FastAPI, HTTPException\n",
        "\n",
        "# Ngrok for exposing local server to the internet\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Needed to allow async FastAPI to run in notebooks like Colab\n",
        "import nest_asyncio\n",
        "\n",
        "# For request validation in FastAPI\n",
        "from pydantic import BaseModel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6260ffb2",
      "metadata": {
        "id": "6260ffb2"
      },
      "source": [
        "## üîê **Hugging Face Authentication**\n",
        "\n",
        "Set up authentication to access private models and repos:\n",
        "\n",
        "- üîë **Save Token**: Store your Hugging Face token securely using `HfFolder.save_token()`.  \n",
        "- üåê **Environment Variable**: Export `HF_TOKEN` so libraries like `transformers` and `langchain` can use it automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39fac91",
      "metadata": {
        "id": "f39fac91"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfFolder\n",
        "HfFolder.save_token(\"hf_YOUR_TOKEN_HERE\")  # Save token locally\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN_HERE\"  # Set token as env variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87855aa",
      "metadata": {
        "id": "a87855aa"
      },
      "source": [
        "## üßæ **Amazon Listing Prompt Template**\n",
        "\n",
        "Set up a powerful prompt to generate high-converting Amazon listings using Mistral:\n",
        "\n",
        "- üõçÔ∏è **Structured Output**: The prompt enforces a strict JSON format with fields like `title`, `bullet_points`, `product_description`, etc.\n",
        "- üéØ **SEO-Optimized Language**: Encourages persuasive, keyword-rich, benefit-driven content tailored for Amazon.\n",
        "- üß† **Smart Formatting Rules**: Follows Amazon listing standards‚Äîcharacter limits, formatting styles, and call-to-actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc10d58f",
      "metadata": {
        "id": "fc10d58f"
      },
      "outputs": [],
      "source": [
        "# üßæ Define the base prompt to guide Mistral in generating a full Amazon listing\n",
        "# The prompt includes detailed structure, formatting, and style instructions\n",
        "\n",
        "\n",
        "listing_prompt = \"\"\"You are an expert ecommerce content strategist and Amazon listing specialist with world-class SEO experience. You always write persuasive, structured, keyword-optimized listings that follow Amazon's best practices and maximize conversions and search rankings.\n",
        "\n",
        "Your task is to write a full Amazon product listing in structured JSON format using the details provided by the user. Use advanced keyword placement, compelling emotional language, and Amazon-specific formatting techniques.\n",
        "\n",
        "EXAMPLE OF EXCELLENT OUTPUT:\n",
        "\n",
        "  \"title\": \"ACME Premium Yoga Mat with Non-Slip Surface for All Levels - Extra Thick 1/2 Inch Cushioned Exercise Mat, Eco-Friendly TPE, Teal\",\n",
        "  \"bullet_points\": [\n",
        "    \"EXTRA THICK CUSHIONING provides ultimate comfort for your joints with 1/2 inch (12.7mm) of high-density padding that protects knees, hips and elbows during floor exercises\",\n",
        "    \"NON-SLIP TEXTURED SURFACE ensures stability and prevents injuries with our specially designed dual-layer grip technology that works on any floor surface\",\n",
        "    \"ECO-FRIENDLY MATERIALS made from 100% recyclable TPE that contains no PVC, latex, or harmful chemicals, making it safe for you and the environment\",\n",
        "    \"PERFECT FOR ALL YOGA STYLES including hot yoga, pilates, and general fitness with moisture-resistant technology that prevents bacteria growth and odors\",\n",
        "    \"LIGHTWEIGHT AND PORTABLE design includes a free carrying strap, making this 72\\\" x 24\\\" mat easy to transport to studio classes or outdoor sessions\"\n",
        "  ],\n",
        "  \"product_description\": \"Transform your yoga practice with the ACME Premium Yoga Mat, engineered for practitioners who demand both comfort and performance. Our revolutionary non-slip surface technology provides exceptional grip even during the most intense hot yoga sessions, eliminating the frustration of slipping mats that disrupt your flow.\\n\\nMeticulously crafted from eco-friendly TPE material, this mat offers superior cushioning at 1/2 inch thickness while remaining surprisingly lightweight. The closed-cell construction prevents moisture absorption, making cleanup effortless and extending the life of your mat far beyond traditional options.\\n\\nWhether you're a beginner or advanced yogi, the ACME Premium Yoga Mat supports your journey with alignment markers subtly incorporated into the elegant design. Experience the perfect balance of stability, comfort and earth-friendly materials that thousands of 5-star reviews can't stop raving about.\",\n",
        "  \"backend_search_terms\": \"yoga mat exercise mat fitness mat workout mat thick cushioned non slip thick extra large tpe eco friendly meditation pilates floor mat home gym equipment exercise equipment beginners advanced hot yoga bikram alignment markers\",\n",
        "  \"attributes\":\n",
        "    \"material\": \"TPE (Thermoplastic Elastomer)\",\n",
        "    \"color\": \"Teal\",\n",
        "    \"size\": \"72 x 24 x 0.5 inches\",\n",
        "    \"target_audience\": \"Adults, All Yoga Levels\",\n",
        "    \"recommended_uses\": \"Yoga, Pilates, Fitness, Meditation, Floor Exercises\",\n",
        "    \"special_features\": \"Non-slip surface, Extra thick, Alignment markers, Eco-friendly\",\n",
        "    \"warranty\": \"1-Year Manufacturer Warranty\",\n",
        "    \"country_of_origin\": \"USA\",\n",
        "    \"weight\": \"2.5 pounds\",\n",
        "    \"dimensions\": \"72 x 24 x 0.5 inches\"\n",
        "\n",
        "===== STRUCTURE FOR YOUR OUTPUT =====\n",
        "\n",
        "You must create a JSON object with these exact fields:\n",
        "- title\n",
        "- bullet_points (array of 5 items)\n",
        "- product_description\n",
        "- backend_search_terms\n",
        "- attributes (object with material, color, size, target_audience, recommended_uses, special_features, warranty, country_of_origin, weight, dimensions)\n",
        "\n",
        "===== DETAILED INSTRUCTIONS =====\n",
        "\n",
        "TITLE REQUIREMENTS:\n",
        "* Begin with the brand name. If not specified, just mention *BRAND_NAME*\n",
        "* Include primary keyword early\n",
        "* Stay under 200 characters\n",
        "* Use strong modifiers (e.g., \"premium\", \"professional\", \"2024 model\")\n",
        "* Format: [BRAND] [PRIMARY KEYWORD] with [KEY FEATURE] for [TARGET AUDIENCE] - [USP], [SIZE/QUANTITY]\n",
        "\n",
        "BULLET POINTS REQUIREMENTS:\n",
        "* Create exactly 5 bullet points\n",
        "* Start each with 2-3 WORDS IN ALL CAPS highlighting a core benefit\n",
        "* Follow with detailed explanation using secondary keywords\n",
        "* Each bullet should be 150-200 characters\n",
        "* Focus on benefits, not just features\n",
        "\n",
        "PRODUCT DESCRIPTION REQUIREMENTS:\n",
        "* Write 2-3 persuasive paragraphs (300-450 words total)\n",
        "* Include emotional triggers and pain points\n",
        "* Incorporate primary and secondary keywords naturally\n",
        "* Use line breaks between paragraphs\n",
        "* End with a call to action\n",
        "\n",
        "BACKEND SEARCH TERMS REQUIREMENTS:\n",
        "* Create a single string of keywords (no commas)\n",
        "* No repeated words or brand names\n",
        "* Maximum 250 characters\n",
        "* Include synonyms, alternate spellings, Misspellings\t and related use cases\n",
        "\n",
        "ATTRIBUTES REQUIREMENTS:\n",
        "* Fill all attribute fields based on product information\n",
        "* If information is missing, make logical assumptions\n",
        "* Be specific and detailed in each attribute\n",
        "\n",
        "===== EXAMPLE STRUCTURE (DO NOT COPY THIS JSON, CREATE YOUR OWN) =====\n",
        "\n",
        "\n",
        "  \"title\": \"*BRAND_NAME*(your title here)\",\n",
        "  \"bullet_points\": [\n",
        "    \"(your first bullet point here)\",\n",
        "    \"(your second bullet point here)\",\n",
        "    \"(your third bullet point here)\",\n",
        "    \"(your fourth bullet point here)\",\n",
        "    \"(your fifth bullet point here)\"\n",
        "  ],\n",
        "  \"product_description\": \"(your product description here)\",\n",
        "  \"backend_search_terms\": \"(your backend search terms here)\",\n",
        "  \"attributes\":\n",
        "    \"material\": \"(material here)\",\n",
        "    \"color\": \"(color here)\",\n",
        "    \"size\": \"(size here)\",\n",
        "    \"target_audience\": \"(target audience here)\",\n",
        "    \"recommended_uses\": \"(recommended uses here)\",\n",
        "    \"special_features\": \"(special features here)\",\n",
        "    \"warranty\": \"(warranty here)\",\n",
        "    \"country_of_origin\": \"(country of origin here)\",\n",
        "    \"weight\": \"(weight here)\",\n",
        "    \"dimensions\": \"(dimensions here)\"\n",
        "\n",
        "\n",
        "\n",
        "Output ONLY the JSON object. No commentary, no explanation.\n",
        "\n",
        "User product details:\n",
        "{product_data}\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7514dee",
      "metadata": {
        "id": "b7514dee"
      },
      "source": [
        "## üöÄ **Model Setup: Mistral 8B with 4-bit Quantization**\n",
        "\n",
        "- ‚öôÔ∏è **Model & Tokenizer**: Load Mistral 8B with efficient quantization.\n",
        "- üîÑ **Pipeline**: Configured for creative text generation.\n",
        "- üß© **Prompt**: Template for product listing generation.\n",
        "- üßπ **Output Cleaning**: Extract JSON from raw text.\n",
        "- üìù **Listing Generation**: Run prompt and return output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db1434f",
      "metadata": {
        "id": "1db1434f"
      },
      "outputs": [],
      "source": [
        "class LLMService:\n",
        "    def __init__(self):\n",
        "        # Initialize model and prompt on creation of the service object\n",
        "        self.setup_models()\n",
        "        self.setup_prompt()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"\n",
        "        Initialize the Mistral 8B model with 4-bit quantization for lower VRAM usage.\n",
        "        Set up the tokenizer and a text-generation pipeline with sampling parameters.\n",
        "        \"\"\"\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Ministral-8B-Instruct-2410\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"mistralai/Ministral-8B-Instruct-2410\",\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=False\n",
        "        )\n",
        "\n",
        "        self.pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device_map=\"auto\",\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_k=40,\n",
        "            top_p=0.9,\n",
        "            max_new_tokens=5000,\n",
        "            repetition_penalty=1.2,\n",
        "            num_return_sequences=1,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        # Wrap pipeline in Langchain wrapper for integration\n",
        "        self.llm = HuggingFacePipeline(pipeline=self.pipeline)\n",
        "\n",
        "    def setup_prompt(self):\n",
        "        \"\"\"\n",
        "        Load and prepare the prompt template for product listing generation.\n",
        "        The prompt expects 'product_data' as input variable.\n",
        "        \"\"\"\n",
        "        self.prompt = PromptTemplate(\n",
        "            template=listing_prompt,  # Your external prompt string\n",
        "            input_variables=[\"product_data\"]\n",
        "        )\n",
        "\n",
        "    def clean_llava_output(self, raw_text: str) -> dict:\n",
        "        \"\"\"\n",
        "        Extract JSON object from raw model output text.\n",
        "\n",
        "        Args:\n",
        "            raw_text (str): Raw string output from the model.\n",
        "\n",
        "        Returns:\n",
        "            dict or None: Parsed JSON if found, else None.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Match a ```json { ... } ``` block and extract the JSON inside\n",
        "            m = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", raw_text, re.DOTALL)\n",
        "            if not m:\n",
        "                return None\n",
        "            json_str = m.group(1)\n",
        "            return json.loads(json_str)\n",
        "        except Exception as e:\n",
        "            print(f\"[clean_llava_output] JSON cleaning failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_listing(self, raw_llava_output: str) -> dict:\n",
        "        \"\"\"\n",
        "        Generate an Amazon product listing by cleaning LLaVA output and\n",
        "        running the cleaned data through the Mistral model.\n",
        "\n",
        "        Args:\n",
        "            raw_llava_output (str): Raw text output from LLaVA model.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains either the raw Mistral output or an error message.\n",
        "        \"\"\"\n",
        "        # Clean raw text to extract valid JSON data\n",
        "        clean_data = self.clean_llava_output(raw_llava_output)\n",
        "        if clean_data is None:\n",
        "            return {\"error\": \"Invalid or missing JSON in LLaVA output\"}\n",
        "\n",
        "        # Create a Langchain LLMChain and run prompt with the cleaned data\n",
        "        chain = LLMChain(prompt=self.prompt, llm=self.llm)\n",
        "        raw_result = chain.run({\"product_data\": json.dumps(clean_data, indent=2)})\n",
        "\n",
        "        # Debug print the raw Mistral output\n",
        "        print(\"\\n=== MISTRAL RAW OUTPUT ===\\n\", raw_result, \"\\n=============================\\n\")\n",
        "\n",
        "        return {\"raw_mistral_output\": raw_result}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e02cd5",
      "metadata": {
        "id": "68e02cd5"
      },
      "source": [
        "## üåê **FastAPI Server & Ngrok Tunnel**\n",
        "\n",
        "- üì• **API Endpoint**: Receives raw LLaVA output, returns Mistral listing.\n",
        "- ‚ö†Ô∏è **Error Handling**: Proper HTTP status on failures.\n",
        "- üîó **Ngrok**: Exposes local server to the internet with a public URL.\n",
        "- üöÄ **Run Server**: Starts FastAPI app with async support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d5ca00",
      "metadata": {
        "id": "c7d5ca00"
      },
      "outputs": [],
      "source": [
        "# Define request schema for incoming data\n",
        "class ListingRequest(BaseModel):\n",
        "    llava_output: str  # Raw output from LLaVA to be processed\n",
        "\n",
        "# Initialize FastAPI app and LLM service instance\n",
        "app = FastAPI()\n",
        "service = LLMService()\n",
        "\n",
        "# API route to generate product listing from LLaVA output\n",
        "@app.post(\"/generate-listing\")\n",
        "async def create_listing(request: ListingRequest):\n",
        "    try:\n",
        "        result = service.generate_listing(request.llava_output)\n",
        "        if \"error\" in result:\n",
        "            # Return 400 for invalid JSON or input errors\n",
        "            raise HTTPException(status_code=400, detail=result[\"error\"])\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        # Return 500 for unexpected server errors\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Ngrok setup to expose local FastAPI server publicly\n",
        "NGROK_TOKEN = \"YOUR-NGROK-TOKEN-HERE\"  # Replace with your actual token\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f'üîó Mistral API URL: {ngrok_tunnel.public_url}')\n",
        "\n",
        "# Allow nested async loops and start Uvicorn server\n",
        "nest_asyncio.apply()\n",
        "import uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
